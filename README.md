# TrekBuddy Guide Text Generator


### í”„ë¡œì íŠ¸ ì†Œê°œ
<a href=https://github.com/Yang-ga-hyeon/TrekBuddy>í•œêµ­ì–´ í…ìŠ¤íŠ¸ ëª¨ë¸ ê¸°ë°˜ ì—¬í–‰ ê°€ì´ë“œ ì–´í”Œë¦¬ì¼€ì´ì…˜ TrekBuddy</a>ì˜ ê°€ì´ë“œ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤.

## â²ê°œë°œ ê¸°ê°„
* 2023.08.01 ~ 2023.11.10

### ğŸ§‘â€ğŸ¤â€ğŸ§‘ë©¤ë²„
- ì¡°ìˆ˜ë¹ˆ: Project Leader, ML Data Collection & Preprocess, App Develop, Database Management, ê°€ì´ë“œ í˜ì´ì§€, ê²€ìƒ‰ í˜ì´ì§€, ìŠ¤í¬ë© í˜ì´ì§€
- ì–‘ê°€í˜„: Main App Developer, Database Management, ë¡œê·¸ì¸ í˜ì´ì§€, íšŒì›ê°€ì… í˜ì´ì§€, ê°€ì´ë“œ í˜ì´ì§€, ê°€ì´ë“œ ìƒì„¸ í˜ì´ì§€
- ê¹€ìœ¤ì„ : Data Preprocess & Model Finetuning, App Develop, Database Management, ê°€ì´ë“œ í˜ì´ì§€, ê°œì¸ì •ë³´ ìˆ˜ì • í˜ì´ì§€, ë¦¬ë·° í˜ì´ì§€, ë¡œê·¸ í˜ì´ì§€

### ğŸ’»ê°œë°œ í™˜ê²½
- **Pretrained-model** : í•œêµ­ì–´ Bart ëª¨ë¸ ì˜¤í”ˆ ì†ŒìŠ¤ (https://github.com/SKT-AI/KoBART) <img src="https://img.shields.io/badge/google colab-F9AB00?style=for-the-badge&logo=google colab&logoColor=white">
- **Training data** : AI hub ìš”ì•½ë¬¸ ë° ë ˆí¬íŠ¸ ìƒì„± ë°ì´í„° (https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=582) <img width="100" height="20" src="https://github.com/Yang-ga-hyeon/TrekBuddy/assets/111068038/f4264824-e151-4a5b-94ea-c257113cf0b8">
- **Deep Learning Framework** :<img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=PyTorch&logoColor=white"> <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=Python&logoColor=white">
- **TTS** : NAVER CLOVA <img width="50" height="50" src="https://github.com/Yang-ga-hyeon/TrekBuddy/assets/111068038/9af716e9-dd8c-411c-bffb-7ab7c03a04b3">
- **Database** : <img src="https://img.shields.io/badge/Firebase-FFCA28?style=for-the-badge&logo=Firebase&logoColor=white">

## ì•„í‚¤í…ì²˜
<img width="500" alt="ì•„í‚¤í…ì²˜" src="https://github.com/Yang-ga-hyeon/TrekBuddy/assets/111068038/2f32b512-d40b-4d81-9c86-66f3da123b4d"> <br/>

<img width="500" alt="ì•„í‚¤í…ì²˜-ê¸°ìˆ " src="https://github.com/Yang-ga-hyeon/TrekBuddy/assets/111068038/3fe28e87-8341-44bc-86b0-77526c2b430d">
  
  *Seleniumì´ ì•„ë‹Œ Scrapyë¡œ ë³€ê²½


## ì„¤ì¹˜ë°©ë²• How to Build
- Directory êµ¬ì¡°
<img width="300" height="400" src="https://github.com/chsubinn/TrekBuddy-Guide-Generator/assets/111068038/280821e7-6258-4488-a3bd-394ffcebff40">

- **Data folder**: train data, test data ë„£ê¸°
- **checkpoint folder**: model binary
- **wandb folder**: Weights & Biases. ëª¨ë¸ì´ í•™ìŠµí•œ ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥, ì‹œìŠ¤í…œ(GPU, CPU, memory ë“±) ëª¨ë‹ˆí„°ë§
- **SKT KoBart open source files** 


## ì‚¬ìš©ë°©ë²• How to Test
**1) ê°œë°œ í™˜ê²½ setting**
  - GPU ì‚¬ìš©
  - SKT KoBart open source download
  - í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ install

**2) Dataset ì¤€ë¹„ & preprocess**
  - training data (his-cul, public ë“±)ì„ ê³ ë¥´ê³  í•´ë‹¹ ë°ì´í„°ë¥¼ csv íŒŒì¼ë¡œ í˜•ì‹ ë³€í™˜, ì‹¤í–‰ ì „ ROOT ê²½ë¡œ ë° csv íŒŒì¼ëª… ì§€ì • í•„ìˆ˜ 
     ```python
       python json2csv.py
     ```

  - csv íŒŒì¼ì„ tsv íŒŒì¼ë¡œ í˜•ì‹ ë³€í™˜
     ```python
       python csvTotsv.py
     ```
**3) Train(finetuning)**
  - dataset.pyì—ì„œ input_idsì™€ label_idsë¥¼ ë³¸ì¸ì˜ ë°ì´í„°ì…‹ì— ë§ê²Œ ë³€ê²½
    ```python
       input_ids = self.tokenizer.encode(instance['passage'])
       label_ids = self.tokenizer.encode(instance['utterance'])      
     ```
  - train.pyì—ì„œ Model checkpoint íŒŒë¼ë¯¸í„° ë³€ê²½
    ```python
       checkpoint_callback = ModelCheckpoint(monitor='val_loss',
                                          dirpath=args.checkpoint,
                                          filename='model_chp/{epoch:02d}-{val_loss:.3f}',
                                          verbose=True,
                                          save_last=False,                                         
                                          mode='min',
                                          save_top_k=3, 
                                          every_n_epochs=25)      
     ```
  - train.pyì— best accurary model checkpoint ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ ì¶”ê°€
    ```python
      #print best_checkpoint path
      best_checkpoint = checkpoint_callback.best_model_path
      print('\n best checkpoint path:', best_checkpoint)    
    ```
   - í•™ìŠµí•˜ê¸°
     ```python
      python train.py --gradient_clip_val 1.0 \
                      --max_epochs 100 \
                      --checkpoint checkpoint \
                      --accelerator gpu \
                      --num_gpus 1 \
                      --batch_size 4 \
                      --num_workers 4
     ```

**4) Evaluation**
  - outputê³¼ ì‚¬ëŒì´ ì§ì ‘ ìš”ì•½í•œ ë¬¸ì¥ì„ ë¹„êµí•˜ì—¬ rouge scoreë¡œ í‰ê°€
     ```python
       python rouge.py
     ```
 - í•™ìŠµí•œ ëª¨ë¸ë¡œ ì¸¡ì •í•œ ê²°ê³¼

    |         |rouge-1|rouge-2|rouge-l|
    |---------|-------|-------|-------|
    |Precision|0.61702|0.42857|0.61702|
    |Recal    |0.58   |0.38182|0.58   |
    |F1       |0.59794|0.40385|0.59794|
    
     
**5) Test**
  - í•™ìŠµí•œ model binary ì¶”ì¶œ
    ```python
       python get_model_binary.py --model_binary 'ì¶”ì¶œí•  checkpoint ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”'
    ```
  - í•™ìŠµí•œ model loadí•˜ì—¬ ì‚¬ìš©
    ```python
       import torch
        from transformers import PreTrainedTokenizerFast
        from transformers.models.bart import BartForConditionalGeneration
        
        # ëª¨ë¸ ë°”ì´ë„ˆë¦¬ íŒŒì¼ ê²½ë¡œ
        model_binary_path = 'model binary ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”'
        
        # KoBART ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
        model = BartForConditionalGeneration.from_pretrained(model_binary_path)

        # ì…ë ¥ í…ìŠ¤íŠ¸
        input_text =  "ìš”ì•½í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”"
        # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬ ì¸ì½”ë”©
        input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=1024, truncation=True)
        
        # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìƒì„±
        summary_ids = model.generate(input_ids, max_length=150, min_length=15, length_penalty=2.0, num_beams=4, early_stopping=True)
        
        # ìš”ì•½ ê²°ê³¼ ë””ì½”ë”©
        summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
        # ìš”ì•½ ì¶œë ¥
        print("ìš”ì•½ í…ìŠ¤íŠ¸:", summary_text)
    ```
    

## Data Description
-  AI hub ìš”ì•½ë¬¸ ë° ë ˆí¬íŠ¸ ìƒì„± ë°ì´í„° ì‚¬ìš©
   (https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=data&dataSetSn=97)
- Data Set êµ¬ì¡°
  ```bash
  â”œâ”€â”€ Meta(Acqusition)
  â”‚   â”œâ”€â”€ doc_id
  â”‚   â”œâ”€â”€ doc_category
  â”‚   â”œâ”€â”€ doc_type
  â”‚   â”œâ”€â”€ doc_name
  â”‚   â”œâ”€â”€ author
  â”‚   â”œâ”€â”€ publisher
  â”‚   â”œâ”€â”€ publisher_year
  â”‚   â””â”€â”€ doc_origin
  â”œâ”€â”€ Meta(Refine)
  â”‚   â”œâ”€â”€ passage_id
  â”‚   â”œâ”€â”€ passage // ì›ë¬¸
  â”‚   â””â”€â”€ passage_Cnt // ì›ë¬¸ ê¸€ì ìˆ˜
  â””â”€â”€ Annotation
      â”œâ”€â”€ summary1 // ì¶”ì¶œ ìš”ì•½
      â”œâ”€â”€ summary2
      â”œâ”€â”€ summary3
      â””â”€â”€ summary_3_cnt // ìƒì„± ìš”ì•½
  ``` 
- json2csv.pyì—ì„œëŠ” passageì™€ summary1, 2, 3ë¥¼ ëª¨ì€ ì¶”ì¶œ ìš”ì•½ ë°ì´í„°ë¥¼ ëª¨ì•„ (passage, summary) ìŒìœ¼ë¡œ ë§Œë“¤ê³  ì´ë¥¼ csvë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- csvTotsv.pyì—ì„œëŠ” json2csv.pyíŒŒì¼ì—ì„œ ë³€í™˜í•œ csvë¥¼ tsvë¡œ ë³€í™˜í•˜ì—¬ ìš”ì•½ ëª¨ë¸ì˜ ë°ì´í„°ë¡œ ì…ë ¥í•©ë‹ˆë‹¤.

